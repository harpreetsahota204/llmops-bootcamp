{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LlamaIndex to Automate the fine-tuning of GPT-3.5-turbo on source documents\n",
    "\n",
    "Primarly Extended from [this](https://colab.research.google.com/drive/1vWeJBXdFEObuihO7Z8ui2CAYkdHQORqo?usp=sharing) notebook, we'll take a look at how we can wrap this process into Chainlit and have our own dynamic fine-tuning machine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q -U llama-index pypdf sentence-transformers ragas openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "openai_api_key = getpass(\"Enter your OpenAI API key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 3322k  100 3322k    0     0   663k      0  0:00:05  0:00:05 --:--:--  783k\n"
     ]
    }
   ],
   "source": [
    "!curl https://jaydixit.com/files/PDFs/TheultimateHitchhikersGuide.pdf --output hitchhikers.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ServiceContext` in LlamaIndex is a bundle of commonly used resources that are used during the indexing and querying stages of a LlamaIndex pipeline or application. \n",
    "\n",
    "It allows you to configure and customize various components of the pipeline. \n",
    "\n",
    "Here is a crash course on the ServiceContext:\n",
    "\n",
    "### Concept:\n",
    "The ServiceContext is a dataclass that contains the following components:\n",
    "\n",
    "• LLM (Language Model): Used to generate natural language responses to queries.\n",
    "\n",
    "• Prompt Helper: Helps with truncating and repacking text chunks to fit in the LLM's context window.\n",
    "\n",
    "• Embedding Model: Generates vector representations of text.\n",
    "\n",
    "• Node Parser: Converts documents into nodes.\n",
    "\n",
    "• Callback Manager: Calls handlers on events and provides logging and tracing capabilities.\n",
    "\n",
    "## Usage Pattern:\n",
    "\n",
    "Configuring the service context:\n",
    "\n",
    "You can directly construct a ServiceContext by passing in the desired components.\n",
    "\n",
    "Alternatively, you can use the `ServiceContext.from_defaults` method to set default values for the components.\n",
    "\n",
    "The `from_defaults`` method also allows you to configure specific modules, such as the LLM, Embedding Model, and Node Parser, by providing additional kwargs.\n",
    "\n",
    "Setting global configuration:\n",
    "\n",
    " - You can set a service context as the global default for the entire LlamaIndex pipeline using the `set_global_service_context` function.\n",
    " - The global service context will be used as the default if not specified as a keyword argument in LlamaIndex functions.\n",
    "\n",
    "Setting local configuration:\n",
    "\n",
    " - You can pass a service context to specific parts of the pipeline to override the default configuration.\n",
    " - For example, when creating a query engine, you can provide a service_context parameter to customize the behavior of the query engine.\n",
    " - By understanding and manipulating the ServiceContext, you can customize the behavior of the indexing process in LlamaIndex to suit your specific needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.evaluation import DatasetGenerator\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"hitchhikers.pdf\"]\n",
    ").load_data()\n",
    "\n",
    "# Shuffle the documents\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(documents)\n",
    "\n",
    "gpt_35_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.3)\n",
    ")\n",
    "\n",
    "question_gen_query = (\n",
    "    \"You are a Teacher/ Professor. Your task is to setup \"\n",
    "    \"a quiz/examination. Using the provided context from a \"\n",
    "    \"report on climate change and the oceans, formulate \"\n",
    "    \"a single question that captures an important fact from the \"\n",
    "    \"context. Restrict the question to the context information provided.\"\n",
    ")\n",
    "\n",
    "dataset_generator = DatasetGenerator.from_documents(\n",
    "    documents[:50],\n",
    "    question_gen_query=question_gen_query,\n",
    "    service_context=gpt_35_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative Questions with `gpt-3.5-turbo`\n",
    "\n",
    "We can use the `generate_questions_from_nodes()` method of our dataset generator to produce a number of questions that will be used to fine-tune!\n",
    "\n",
    "> NOTE: This cell will take ~30s-2min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated  40  questions\n"
     ]
    }
   ],
   "source": [
    "questions = dataset_generator.generate_questions_from_nodes(num=40)\n",
    "print(\"Generated \", len(questions), \" questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peek and see what was created!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What did Zaphod find on the external monitor screens in the Horsehead Nebula?'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can save our questions into a text file for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_questions.txt\", \"w\") as f:\n",
    "    for question in questions:\n",
    "        f.write(question + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Generator\n",
    "\n",
    "Let's generate questions from a different segment of our documents in order to build a robust test for our RAQA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_generator = DatasetGenerator.from_documents(\n",
    "    documents[\n",
    "        50:\n",
    "    ],  # since we generated ~1 question for 40 documents, we can skip the first 40\n",
    "    question_gen_query=question_gen_query,\n",
    "    service_context=gpt_35_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we'll use `gpt-3.5-turbo` to generate some questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated  40  questions\n"
     ]
    }
   ],
   "source": [
    "questions = dataset_generator.generate_questions_from_nodes(num=40)\n",
    "print(\"Generated \", len(questions), \" questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can save our results for evaluations later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"eval_questions.txt\", \"w\") as f:\n",
    "    for question in questions:\n",
    "        f.write(question + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating base `gpt-3.5-turbo`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load up our evaluation questions and get to it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "with open(\"eval_questions.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        questions.append(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell is constructing our `VectorIndex` so we can move onto testing the base model.\n",
    "\n",
    "This code sets up a vector store index using the `VectorStoreIndex` class, configures the service context with the OpenAI GPT-3.5 Turbo model and a context window size of 2048 tokens, and creates a query engine for performing similarity-based searches on the index. \n",
    "\n",
    "The query engine allows you to perform similarity-based searches on the index and retrieve the most relevant results for a given query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "# limit the context window to 2048 tokens so that refine is used\n",
    "gpt_35_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.3), context_window=2048\n",
    ")\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents, service_context=gpt_35_context)\n",
    "\n",
    "query_engine = index.as_query_engine(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where we're actually putting the model to the test!\n",
    "\n",
    "Let's loop over the list of questions. \n",
    "\n",
    "For each question, uses the `query_engine` to query for a response. `query_engine.query` compares the query against the indexed documents, calculates similarity scores, ranks the documents, and returns the most relevant results based on the specified similarity_top_k value. \n",
    "\n",
    "The `query_engine.query` method takes a query string or document as input.\n",
    "\n",
    "The query engine processes the query by converting it into a vector representation using the same embedding model that was used during the indexing process. This vector representation captures the semantic meaning of the query. The query engine then compares the vector representation of the query against the vector representations of the indexed documents using a similarity metric, such as cosine similarity. \n",
    "\n",
    "This metric calculates the similarity score between the query and each indexed document.\n",
    "\n",
    "Based on the similarity scores, the query engine ranks the indexed documents in descending order, with the most similar documents receiving higher ranks. The query engine returns the top-k most similar results based on the similarity scores. The value of k is determined by the `similarity_top_k` parameter that was set when creating the query engine.\n",
    "\n",
    "### `contexts.append([x.node.get_content() for x in response.source_nodes])`` is responsible for appending the content of each source node to the contexts list.\n",
    "\n",
    "The response includes the source nodes, which are the relevant documents that were used to generate the answer, and the answer itself. The source nodes are stored in the contexts list, and the answers are stored in the answers list.\n",
    "\n",
    "\n",
    "Let's break down the code:\n",
    "\n",
    "• `response.source_nodes` refers to the list of source nodes returned by the query engine in response to a question.\n",
    "\n",
    "• The code uses a list comprehension to iterate over each source node in `response.source_nodes`.\n",
    "\n",
    "• For each source node, `x.node.get_content()`` is called to retrieve the content of the node.\n",
    "\n",
    "• The content of each source node is then appended to a new list, which is then appended to the contexts list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = []\n",
    "answers = []\n",
    "\n",
    "for question in questions:\n",
    "    response = query_engine.query(question)\n",
    "    contexts.append([x.node.get_content() for x in response.source_nodes])\n",
    "    answers.append(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There  had  been  a  small  number  of  significant  letters  in  the  piles  of  \n",
      "junk  ʹ  some  documents  from  the  council,  dated  three  years  earlier,  \n",
      "relating  to  the  proposed  demolition  of  his  house,  and  some  other  \n",
      "letters  about  the  setting  up  of  a  public  inquiry  into  the  whole  bypass  \n",
      "scheme  in  the  area;  there  was  also  an  old  letter  from  Greenp eace,  the  \n",
      "ecological  pressure  group  to  which  he  occasionally  made  \n",
      "contributions,  asking  for  help  with  their  scheme  to  release  dolphins  \n",
      "and  orcas  from  captivity,  and  some  postcards  from  friends,  vaguely  \n",
      "complaining  that  he  never  got  in  touch  these  days.\n"
     ]
    }
   ],
   "source": [
    "print(response.source_nodes[0].node.get_content())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What number did Deep Thought give as the Ultimate Answer in the context provided?'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Yeah,\"  he  said,  \"only  there  was  a  man  in  a  mountain  wanted  to  \n",
      "see  you.\"\"I  met  him.\"\"Yeah,  only  he  seemed  pretty  anxious  about  it,  you  know.\"\"Yes,  I  met  him.\"\"Yeah,  well  I  think  you  should  know  that.\"\"I  do.I  met  him.\"The  man  paused  to  chew  a  little  gum.Then  he  clapped  Arthur  on  \n",
      "the  back.\"OK,\"  he  said,  \"all  right.I'm  just  telling  you,  right?Good  night,  good  \n",
      "luck,  win  awards.\"\"What?\"said  Arthur,  who  was  beginning  to  flounder  seriously  at  \n",
      "this  point.\"Whatever.Do  what  you  do.Do  it  well.\"He  made  a  sort  of  clucking  \n",
      "noise  with  whatever  he  was  chewing  and  then  some  vaguely  dynamic  \n",
      "gesture.\"Why?\"said  Arthur.\"Do  it  badly,\"  said  the  man,  \"who  cares?Who  gives  a  shit?\"The  \n",
      "blood  suddenly  seemed   to  pump  angrily  into  the  man's  face  and  he  \n",
      "started  to  shout.\"Why  not  go  mad?\"he  said.\"Go  away,  get  off  my  back  will  you,  \n",
      "guy.Just  zark  off!!!\"\"OK,  I'm  going,\"  said  Arthur  hurriedly.\"It's  been  real.\"The  man  gave  a  sharp  wave  and  disappeared  off  \n",
      "into  the  throng.\"What  was  that  about?\"said  Arthur  to  a  girl  he  found  standing  \n",
      "beside  him.\"Why  did  he  tell  me  to  win  awards?\"\"Just  showbiz  talk,\"  shrugged  the  girl.\"He's  just  won  an  award  at  \n",
      "the  Annual  Ursa  Minor  Alpha  Recreational  Illusions  Institute   Awards  \n",
      "Ceremony,  and  was  hoping  to  be  able  to  pass  it  off  lightly,  only  you  \n",
      "didn't  mention  it,  so  he  couldn't.\"\"Oh,\"  said  Arthur,  \"oh,  well  I'm  sorry  I  didn't.What  was  it  for?\"\"The  Most  Gratuitous  Use  Of  The  Word  'Fuck'  In  A  Serious  \n",
      "Screenplay.It's   very  prestigious.\"\"I  see,\"  said  Arthur,  \"yes,  and  what  do  you  get  for  that?\"\n"
     ]
    }
   ],
   "source": [
    "print(contexts[32][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The character received the award for \"The Most Gratuitous Use Of The Word \\'Fuck\\' In A Serious Screenplay.\"'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[32]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've tested our model - let's evaluate it to see how it performed!\n",
    "\n",
    "We're testing our model with the `ragas` framework - found [here](https://github.com/explodinggradients/ragas)\n",
    "\n",
    "You'll notice that we're testing two primary metrics:\n",
    "\n",
    "- [`answer_relevancy`](https://github.com/explodinggradients/ragas/blob/a55c3be8b2389501c5c761df9070126027a4d1d6/src/ragas/metrics/answer_relevance.py#L32): This measures how relevant is the generated answer to the prompt. If the generated answer is incomplete or contains redundant information the score will be low. This is quantified by working out the chance of an LLM generating the given question using the generated answer. Values range (0,1), higher the better.\n",
    "- [`faithfulness`](https://github.com/explodinggradients/ragas/blob/a55c3be8b2389501c5c761df9070126027a4d1d6/src/ragas/metrics/faithfulnes.py#L63): This measures the factual consistency of the generated answer against the given context. This is done using a multi step paradigm that includes creation of statements from the generated answer followed by verifying each of these statements against the context. The answer is scaled to (0,1) range. Higher the better.\n",
    "\n",
    "Read more about their implementations [here](https://github.com/explodinggradients/ragas/blob/main/docs/metrics.md)\n",
    "\n",
    "Again, these cells might take some time to complete - be patient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harpreetsahota/miniconda3/envs/aimakerspace/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [01:19<00:00, 26.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [faithfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [05:44<00:00, 114.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ragas_score': 0.8777, 'answer_relevancy': 0.9246, 'faithfulness': 0.8352}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_relevancy, faithfulness\n",
    "\n",
    "ds = Dataset.from_dict(\n",
    "    {\n",
    "        \"question\": questions,\n",
    "        \"answer\": answers,\n",
    "        \"contexts\": contexts,\n",
    "    }\n",
    ")\n",
    "\n",
    "result = evaluate(ds, [answer_relevancy, faithfulness])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_eval = {'ragas_score': 0.8777, 'answer_relevancy': 0.9246, 'faithfulness': 0.8352}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leveraging `gpt-4` to improve our `gpt-3.5-turbo` base model!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.callbacks import OpenAIFineTuningHandler\n",
    "from llama_index.callbacks import CallbackManager\n",
    "\n",
    "finetuning_handler = OpenAIFineTuningHandler()\n",
    "callback_manager = CallbackManager([finetuning_handler])\n",
    "\n",
    "gpt_4_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=\"gpt-4\", temperature=0.3),\n",
    "    context_window=2048,  # limit the context window artifically to test refine process\n",
    "    callback_manager=callback_manager,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "with open(\"train_questions.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        questions.append(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code sets up a vector store index using the `VectorStoreIndex` class, configures the service context with the OpenAI GPT-3.5 Turbo model and a context window size of 2048 tokens, and creates a query engine for performing similarity-based searches on the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents, service_context=gpt_4_context)\n",
    "\n",
    "query_engine = index.as_query_engine(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this process will take a few minutes. \n",
    "\n",
    "While this is a powerful technique - it is unfortunately quite slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in questions:\n",
    "    response = query_engine.query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response='The name of the ecological pressure group mentioned in the context is Greenpeace.', source_nodes=[NodeWithScore(node=TextNode(id_='81fe0d42-e805-45e6-a351-8d32241c7b1c', embedding=None, metadata={'page_label': '572', 'file_name': 'hitchhikers.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='91adb964-c6d6-402c-a798-fb29e0f9b9a4', node_type=None, metadata={'page_label': '572', 'file_name': 'hitchhikers.pdf'}, hash='c65b6a2f21376ac9c4e42fe472ee48394ff1998a5e77446c4bbc59669f1bbe15'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='50e8219c-e80a-433a-a924-19de9fe22e65', node_type=None, metadata={'page_label': '572', 'file_name': 'hitchhikers.pdf'}, hash='1c3c040adc656eeea3a66eedb3377bdcc59bb6c6302c7c7cb456f86eba7ad9d4')}, hash='bca507c26119c357d38f8e2d99577865a71ce9edd1f6d09849ce1d879d923c7a', text='There \\xa0had \\xa0been \\xa0a \\xa0small \\xa0number \\xa0of \\xa0significant \\xa0letters \\xa0in \\xa0the \\xa0piles \\xa0of \\xa0\\njunk \\xa0ʹ \\xa0some \\xa0documents \\xa0from \\xa0the \\xa0council, \\xa0dated \\xa0three \\xa0years \\xa0earlier, \\xa0\\nrelating \\xa0to \\xa0the \\xa0proposed \\xa0demolition \\xa0of \\xa0his \\xa0house, \\xa0and \\xa0some \\xa0other \\xa0\\nletters \\xa0about \\xa0the \\xa0setting \\xa0up \\xa0of \\xa0a \\xa0public \\xa0inquiry \\xa0into \\xa0the \\xa0whole \\xa0bypass \\xa0\\nscheme \\xa0in \\xa0the \\xa0area; \\xa0there \\xa0was \\xa0also \\xa0an \\xa0old \\xa0letter \\xa0from \\xa0Greenp eace, \\xa0the \\xa0\\necological \\xa0pressure \\xa0group \\xa0to \\xa0which \\xa0he \\xa0occasionally \\xa0made \\xa0\\ncontributions, \\xa0asking \\xa0for \\xa0help \\xa0with \\xa0their \\xa0scheme \\xa0to \\xa0release \\xa0dolphins \\xa0\\nand \\xa0orcas \\xa0from \\xa0captivity, \\xa0and \\xa0some \\xa0postcards \\xa0from \\xa0friends, \\xa0vaguely \\xa0\\ncomplaining \\xa0that \\xa0he \\xa0never \\xa0got \\xa0in \\xa0touch \\xa0these \\xa0days.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.7931936515866523), NodeWithScore(node=TextNode(id_='d774e58f-4580-488d-95b4-cc1d2bc666eb', embedding=None, metadata={'page_label': '444', 'file_name': 'hitchhikers.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='3d336980-eaec-4d31-82b6-5ebcb782efdf', node_type=None, metadata={'page_label': '444', 'file_name': 'hitchhikers.pdf'}, hash='eb56c3ba5ca4e3840ad37cae188c0d31326c8f3eb522f8a1d7529fe8c0978447'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='4ccaed10-8f50-444f-904e-bbd2974ac98c', node_type=None, metadata={'page_label': '444', 'file_name': 'hitchhikers.pdf'}, hash='f17fb00b7e697ef9c3275c29633e8ea217d43f41407a56fc36ded48b764ffb18')}, hash='7c9740fda4b2ed9ad19613f2265cd6a0536db16a007cea81053a7a63d8397011', text=\"They \\xa0\\naren't \\xa0quite \\xa0sure \\xa0what \\xa0the \\xa0point \\xa0is, \\xa0but \\xa0they \\xa0are \\xa0quite \\xa0sure \\xa0that \\xa0that \\xa0\\nisn't \\xa0it.They \\xa0set \\xa0up \\xa0the \\xa0Campaign \\xa0for \\xa0Real \\xa0Time \\xa0to \\xa0try \\xa0to \\xa0stop \\xa0this \\xa0sort \\xa0\\nof \\xa0thing \\xa0going \\xa0on.Their \\xa0case \\xa0was \\xa0considerably \\xa0strengthened \\xa0by \\xa0the \\xa0\\nfact \\xa0that \\xa0a \\xa0week \\xa0after \\xa0they \\xa0had \\xa0set \\xa0themselves \\xa0 up, \\xa0news \\xa0broke \\xa0that \\xa0\\nnot \\xa0only \\xa0had \\xa0the \\xa0great \\xa0Cathedral \\xa0of \\xa0Chalesm \\xa0been \\xa0pulled \\xa0down \\xa0in \\xa0\\norder \\xa0to \\xa0build \\xa0a \\xa0new \\xa0ion \\xa0refinery, \\xa0but \\xa0that \\xa0the \\xa0construction \\xa0of \\xa0the \\xa0\\nrefinery \\xa0had \\xa0taken \\xa0so \\xa0long, \\xa0and \\xa0had \\xa0had \\xa0to \\xa0extend \\xa0so \\xa0far \\xa0back \\xa0into \\xa0the \\xa0\\npast \\xa0in \\xa0order \\xa0to \\xa0allow \\xa0ion \\xa0produc tion \\xa0to \\xa0start \\xa0on \\xa0time, \\xa0that \\xa0the \\xa0\\nCathedral \\xa0of \\xa0Chalesm \\xa0had \\xa0now \\xa0never \\xa0been \\xa0built \\xa0in \\xa0the \\xa0first \\xa0place.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.7730488652133298)], metadata={'81fe0d42-e805-45e6-a351-8d32241c7b1c': {'page_label': '572', 'file_name': 'hitchhikers.pdf'}, 'd774e58f-4580-488d-95b4-cc1d2bc666eb': {'page_label': '444', 'file_name': 'hitchhikers.pdf'}})"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the fine-tuning dataset\n",
    "\n",
    "Now that we have a number of fine-tuning events from our `OpenAIFineTuningHandler()`, let's save them to a `.jsonl` file - the expected format for fine-tuning `gpt-3.5-turbo`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 47 examples to finetuning_events.jsonl\n"
     ]
    }
   ],
   "source": [
    "finetuning_handler.save_finetuning_events(\"finetuning_events.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "file_response = openai.File.create(file=open(\"finetuning_events.jsonl\", \"rb\"), purpose='fine-tune')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<File file id=file-e1DT1AWzFHjqntBwPXHOdirC at 0x1615c9670> JSON: {\n",
       "  \"object\": \"file\",\n",
       "  \"id\": \"file-e1DT1AWzFHjqntBwPXHOdirC\",\n",
       "  \"purpose\": \"fine-tune\",\n",
       "  \"filename\": \"file\",\n",
       "  \"bytes\": 200900,\n",
       "  \"created_at\": 1693080563,\n",
       "  \"status\": \"uploaded\",\n",
       "  \"status_details\": null\n",
       "}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "response = None\n",
    "\n",
    "while not response:\n",
    "  try:\n",
    "    response = openai.FineTuningJob.create(training_file=file_response.id, model=\"gpt-3.5-turbo\")\n",
    "  except:\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FineTuningJob fine_tuning.job id=ftjob-Uz1vMnKp6RVIhYcPzxsXC6GK at 0x284430590> JSON: {\n",
       "  \"object\": \"fine_tuning.job\",\n",
       "  \"id\": \"ftjob-Uz1vMnKp6RVIhYcPzxsXC6GK\",\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"created_at\": 1693080603,\n",
       "  \"finished_at\": null,\n",
       "  \"fine_tuned_model\": null,\n",
       "  \"organization_id\": \"org-uVzc7jXuBij843gam1Xdonxx\",\n",
       "  \"result_files\": [],\n",
       "  \"status\": \"created\",\n",
       "  \"validation_file\": null,\n",
       "  \"training_file\": \"file-e1DT1AWzFHjqntBwPXHOdirC\",\n",
       "  \"hyperparameters\": {\n",
       "    \"n_epochs\": 3\n",
       "  },\n",
       "  \"trained_tokens\": null\n",
       "}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_id = response.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FineTuningJob fine_tuning.job id=ftjob-Uz1vMnKp6RVIhYcPzxsXC6GK at 0x28678daf0> JSON: {\n",
       "  \"object\": \"fine_tuning.job\",\n",
       "  \"id\": \"ftjob-Uz1vMnKp6RVIhYcPzxsXC6GK\",\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"created_at\": 1693080603,\n",
       "  \"finished_at\": null,\n",
       "  \"fine_tuned_model\": null,\n",
       "  \"organization_id\": \"org-uVzc7jXuBij843gam1Xdonxx\",\n",
       "  \"result_files\": [],\n",
       "  \"status\": \"running\",\n",
       "  \"validation_file\": null,\n",
       "  \"training_file\": \"file-e1DT1AWzFHjqntBwPXHOdirC\",\n",
       "  \"hyperparameters\": {\n",
       "    \"n_epochs\": 3\n",
       "  },\n",
       "  \"trained_tokens\": null\n",
       "}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.FineTuningJob.retrieve(training_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"object\": \"list\",\n",
      "  \"data\": [\n",
      "    {\n",
      "      \"object\": \"fine_tuning.job.event\",\n",
      "      \"id\": \"ftevent-q8c2gpgXa0NHfLpZ4n6BOLzF\",\n",
      "      \"created_at\": 1693081421,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Fine-tuning job successfully completed\",\n",
      "      \"data\": null,\n",
      "      \"type\": \"message\"\n",
      "    },\n",
      "    {\n",
      "      \"object\": \"fine_tuning.job.event\",\n",
      "      \"id\": \"ftevent-V3C4WMqxuYCTgN7QNNmI2WAo\",\n",
      "      \"created_at\": 1693081419,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"New fine-tuned model created: ft:gpt-3.5-turbo-0613:personal::7ru6l1bi\",\n",
      "      \"data\": null,\n",
      "      \"type\": \"message\"\n",
      "    },\n",
      "    {\n",
      "      \"object\": \"fine_tuning.job.event\",\n",
      "      \"id\": \"ftevent-tXV7SEGD65xeJ8Ypb683PURc\",\n",
      "      \"created_at\": 1693081412,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Step 140/141: training loss=0.16\",\n",
      "      \"data\": {\n",
      "        \"step\": 140,\n",
      "        \"train_loss\": 0.1555028259754181,\n",
      "        \"train_mean_token_accuracy\": 0.9485294222831726\n",
      "      },\n",
      "      \"type\": \"metrics\"\n",
      "    },\n",
      "    {\n",
      "      \"object\": \"fine_tuning.job.event\",\n",
      "      \"id\": \"ftevent-J5k4SHH4HluGOpl9OqzX63zO\",\n",
      "      \"created_at\": 1693081398,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Step 130/141: training loss=0.02\",\n",
      "      \"data\": {\n",
      "        \"step\": 130,\n",
      "        \"train_loss\": 0.02389608323574066,\n",
      "        \"train_mean_token_accuracy\": 1.0\n",
      "      },\n",
      "      \"type\": \"metrics\"\n",
      "    },\n",
      "    {\n",
      "      \"object\": \"fine_tuning.job.event\",\n",
      "      \"id\": \"ftevent-yaZuccR5URtyQJ018hfjjhUh\",\n",
      "      \"created_at\": 1693081384,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Step 120/141: training loss=0.14\",\n",
      "      \"data\": {\n",
      "        \"step\": 120,\n",
      "        \"train_loss\": 0.13701018691062927,\n",
      "        \"train_mean_token_accuracy\": 0.9599999785423279\n",
      "      },\n",
      "      \"type\": \"metrics\"\n",
      "    },\n",
      "    {\n",
      "      \"object\": \"fine_tuning.job.event\",\n",
      "      \"id\": \"ftevent-1XapNdtpkiMNeUGqSFdzuxlo\",\n",
      "      \"created_at\": 1693081370,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Step 110/141: training loss=0.05\",\n",
      "      \"data\": {\n",
      "        \"step\": 110,\n",
      "        \"train_loss\": 0.04677148163318634,\n",
      "        \"train_mean_token_accuracy\": 1.0\n",
      "      },\n",
      "      \"type\": \"metrics\"\n",
      "    },\n",
      "    {\n",
      "      \"object\": \"fine_tuning.job.event\",\n",
      "      \"id\": \"ftevent-5bhPomgQ6UHYUzFYqIIQwNRF\",\n",
      "      \"created_at\": 1693081356,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Step 100/141: training loss=0.30\",\n",
      "      \"data\": {\n",
      "        \"step\": 100,\n",
      "        \"train_loss\": 0.3016435503959656,\n",
      "        \"train_mean_token_accuracy\": 0.8333333134651184\n",
      "      },\n",
      "      \"type\": \"metrics\"\n",
      "    },\n",
      "    {\n",
      "      \"object\": \"fine_tuning.job.event\",\n",
      "      \"id\": \"ftevent-Tqqbz5u6tGj9WSNIfxbRS9or\",\n",
      "      \"created_at\": 1693081342,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Step 90/141: training loss=0.07\",\n",
      "      \"data\": {\n",
      "        \"step\": 90,\n",
      "        \"train_loss\": 0.06552088260650635,\n",
      "        \"train_mean_token_accuracy\": 0.9375\n",
      "      },\n",
      "      \"type\": \"metrics\"\n",
      "    },\n",
      "    {\n",
      "      \"object\": \"fine_tuning.job.event\",\n",
      "      \"id\": \"ftevent-yG0DxUJssSo1tLo6oW7Gx0ef\",\n",
      "      \"created_at\": 1693081329,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Step 80/141: training loss=0.24\",\n",
      "      \"data\": {\n",
      "        \"step\": 80,\n",
      "        \"train_loss\": 0.23569917678833008,\n",
      "        \"train_mean_token_accuracy\": 0.8999999761581421\n",
      "      },\n",
      "      \"type\": \"metrics\"\n",
      "    },\n",
      "    {\n",
      "      \"object\": \"fine_tuning.job.event\",\n",
      "      \"id\": \"ftevent-p4nyfsj06mHRYr2nlEwCCS5C\",\n",
      "      \"created_at\": 1693081314,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Step 70/141: training loss=0.03\",\n",
      "      \"data\": {\n",
      "        \"step\": 70,\n",
      "        \"train_loss\": 0.027516432106494904,\n",
      "        \"train_mean_token_accuracy\": 1.0\n",
      "      },\n",
      "      \"type\": \"metrics\"\n",
      "    }\n",
      "  ],\n",
      "  \"has_more\": true\n",
      "}\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "while openai.FineTuningJob.retrieve(training_id).status == \"running\":\n",
    "  clear_output(wait=True)\n",
    "  time.sleep(5)\n",
    "  print(openai.FineTuningJob.list_events(id=training_id, limit=10))\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FineTuningJob fine_tuning.job id=ftjob-Uz1vMnKp6RVIhYcPzxsXC6GK at 0x2842931d0> JSON: {\n",
       "  \"object\": \"fine_tuning.job\",\n",
       "  \"id\": \"ftjob-Uz1vMnKp6RVIhYcPzxsXC6GK\",\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"created_at\": 1693080603,\n",
       "  \"finished_at\": 1693081421,\n",
       "  \"fine_tuned_model\": \"ft:gpt-3.5-turbo-0613:personal::7ru6l1bi\",\n",
       "  \"organization_id\": \"org-uVzc7jXuBij843gam1Xdonxx\",\n",
       "  \"result_files\": [\n",
       "    \"file-vDEXotv2n2fQhJhK4PXnTF3M\"\n",
       "  ],\n",
       "  \"status\": \"succeeded\",\n",
       "  \"validation_file\": null,\n",
       "  \"training_file\": \"file-e1DT1AWzFHjqntBwPXHOdirC\",\n",
       "  \"hyperparameters\": {\n",
       "    \"n_epochs\": 3\n",
       "  },\n",
       "  \"trained_tokens\": 160344\n",
       "}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.FineTuningJob.retrieve(training_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model_id = openai.FineTuningJob.retrieve(training_id).fine_tuned_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the fine-tuned model\n",
    "\n",
    "Now that we've fine-tuned our model on the `gpt-4` enhanced question answers - let's see how it performs on our `raga` evaluation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.callbacks import OpenAIFineTuningHandler\n",
    "from llama_index.callbacks import CallbackManager\n",
    "\n",
    "\n",
    "ft_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=ft_model_id, temperature=0.3),\n",
    "    context_window=2048,  # limit the context window artifically to test refine process\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "with open(\"eval_questions.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        questions.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents, service_context=ft_context)\n",
    "\n",
    "query_engine = index.as_query_engine(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = []\n",
    "answers = []\n",
    "\n",
    "for question in questions:\n",
    "    response = query_engine.query(question)\n",
    "    contexts.append([x.node.get_content() for x in response.source_nodes])\n",
    "    answers.append(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:53<00:00, 17.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [faithfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [03:05<00:00, 61.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ragas_score': 0.8092, 'answer_relevancy': 0.9400, 'faithfulness': 0.7104}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_relevancy, faithfulness\n",
    "\n",
    "ds = Dataset.from_dict(\n",
    "    {\n",
    "        \"question\": questions,\n",
    "        \"answer\": answers,\n",
    "        \"contexts\": contexts,\n",
    "    }\n",
    ")\n",
    "\n",
    "result = evaluate(ds, [answer_relevancy, faithfulness])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_eval = {'ragas_score': 0.8092, 'answer_relevancy': 0.9400, 'faithfulness': 0.7104}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ragas_score': 0.8777, 'answer_relevancy': 0.9246, 'faithfulness': 0.8352}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Differences\n",
    "\n",
    "Now we can compare the outputs of the two models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "with open(\"eval_questions.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        questions.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What did the bird claim reverse engineering enables them to do?\n"
     ]
    }
   ],
   "source": [
    "print(questions[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.response.notebook_utils import display_response\n",
    "from llama_index import ServiceContext\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "\n",
    "gpt_35_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.3),\n",
    "    context_window=2048,  # limit the context window artifically to test refine process\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** The bird claimed that reverse engineering enables them to quickly analyze and understand the technology of a spaceship. This allows them to anticipate when a spaceship will arrive and confidently secure a lift without depending on the spaceship's discretion."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(service_context=gpt_35_context)\n",
    "\n",
    "response = query_engine.query(questions[12])\n",
    "\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "\n",
    "ft_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=ft_model_id, temperature=0.3),\n",
    "    context_window=2048,  # limit the context window artifically to test refine process\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** The bird claimed that reverse engineering allows them to bypass the need to wait for a spaceship to pass through their galactic sector and evaluate whether or not to offer someone a ride. Instead, they can ascertain that a spaceship will provide transportation and facilitate it."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(service_context=ft_context)\n",
    "\n",
    "response = query_engine.query(questions[12])\n",
    "\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model answer_relevancy : 0.9246\n",
      "Fine-tuned model answer_relevancy : 0.94\n",
      "Improvement answer_relevancy : 1.54%\n",
      "\n",
      "Base model faithfulness : 0.8352\n",
      "Fine-tuned model faithfulness : 0.7104\n",
      "Improvement faithfulness : -12.48%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metric_list = [\"answer_relevancy\", \"faithfulness\"]\n",
    "\n",
    "for metric in metric_list:\n",
    "  print(\"Base model\", metric, \":\", base_eval[metric])\n",
    "  print(\"Fine-tuned model\", metric, \":\", ft_eval[metric])\n",
    "  print(f\"Improvement {metric} : {(ft_eval[metric] - base_eval[metric])*100:.2f}%\")\n",
    "  print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aims-barbenheimer-chainlit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
